{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb41af0-d2d4-4c9c-9e7d-ab0d1db94596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:54.007286Z",
     "iopub.status.busy": "2023-04-19T11:20:54.006971Z",
     "iopub.status.idle": "2023-04-19T11:20:54.011644Z",
     "shell.execute_reply": "2023-04-19T11:20:54.010618Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/shoji9x9/CIFAR-10-By-small-ResNet/blob/master/ResNet-for-CIFAR-10-with-Keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fd7612-8606-44ff-9fc3-aa4c8e848cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:54.015389Z",
     "iopub.status.busy": "2023-04-19T11:20:54.015092Z",
     "iopub.status.idle": "2023-04-19T11:20:56.027711Z",
     "shell.execute_reply": "2023-04-19T11:20:56.026339Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import git\n",
    "import pathlib\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "PROJ_ROOT_PATH = pathlib.Path(git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "PROJ_ROOT =  str(PROJ_ROOT_PATH)\n",
    "if PROJ_ROOT not in sys.path:\n",
    "    sys.path.append(PROJ_ROOT)\n",
    "\n",
    "from libs.constants import MODELS_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5127b421-8963-4d68-bd72-293be41d7fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.034562Z",
     "iopub.status.busy": "2023-04-19T11:20:56.033833Z",
     "iopub.status.idle": "2023-04-19T11:20:56.082569Z",
     "shell.execute_reply": "2023-04-19T11:20:56.081653Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit GPU growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fec3701-0818-4940-adc5-03aced736861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.085159Z",
     "iopub.status.busy": "2023-04-19T11:20:56.084954Z",
     "iopub.status.idle": "2023-04-19T11:20:56.090363Z",
     "shell.execute_reply": "2023-04-19T11:20:56.089684Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import libs.model_archs\n",
    "import libs.utils\n",
    "from libs.seeds import load_model_seeds\n",
    "model_seeds = load_model_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f08e42-236b-4329-92ff-92e811be298c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.092843Z",
     "iopub.status.busy": "2023-04-19T11:20:56.092586Z",
     "iopub.status.idle": "2023-04-19T11:20:56.095991Z",
     "shell.execute_reply": "2023-04-19T11:20:56.095350Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataset and model architecture\n",
    "dataset = \"cifar10\"\n",
    "# Select ResNet Version\n",
    "resnet_version = 1\n",
    "model_arch = \"resnetA\" + str(resnet_version)\n",
    "\n",
    "# set training hyperparameters\n",
    "batch_size = 128\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429bb6cd-559b-4edf-8ce6-b51721b7f5c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.098517Z",
     "iopub.status.busy": "2023-04-19T11:20:56.098258Z",
     "iopub.status.idle": "2023-04-19T11:20:56.107435Z",
     "shell.execute_reply": "2023-04-19T11:20:56.106757Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    def __init__(self, cutout_mask_size = 0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cutout_mask_size = cutout_mask_size\n",
    "        \n",
    "    def cutout(self, x, y):\n",
    "        return np.array(list(map(self._cutout, x))), y\n",
    "    \n",
    "    def _cutout(self, image_origin):\n",
    "        # 最後に使うfill()は元の画像を書き換えるので、コピーしておく\n",
    "        image = np.copy(image_origin)\n",
    "        mask_value = image.mean()\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        # マスクをかける場所のtop, leftをランダムに決める\n",
    "        # はみ出すことを許すので、0以上ではなく負の値もとる(最大mask_size // 2はみ出す)\n",
    "        top = np.random.randint(0 - self.cutout_mask_size // 2, h - self.cutout_mask_size)\n",
    "        left = np.random.randint(0 - self.cutout_mask_size // 2, w - self.cutout_mask_size)\n",
    "        bottom = top + self.cutout_mask_size\n",
    "        right = left + self.cutout_mask_size\n",
    "\n",
    "        # はみ出した場合の処理\n",
    "        if top < 0:\n",
    "            top = 0\n",
    "        if left < 0:\n",
    "            left = 0\n",
    "\n",
    "        # マスク部分の画素値を平均値で埋める\n",
    "        image[top:bottom, left:right, :].fill(mask_value)\n",
    "        return image\n",
    "    \n",
    "    def flow(self, *args, **kwargs):\n",
    "        batches = super().flow(*args, **kwargs)\n",
    "        \n",
    "        # 拡張処理\n",
    "        while True:\n",
    "            batch_x, batch_y = next(batches)\n",
    "            \n",
    "            if self.cutout_mask_size > 0:\n",
    "                result = self.cutout(batch_x, batch_y)\n",
    "                batch_x, batch_y = result                        \n",
    "                \n",
    "            yield (batch_x, batch_y)     \n",
    "\n",
    "datagen_parameters = {\"horizontal_flip\": True, \"width_shift_range\": 0.1, \"height_shift_range\": 0.1, \"cutout_mask_size\": 16}\n",
    "datagen = CustomImageDataGenerator(**datagen_parameters)\n",
    "datagen_for_test = ImageDataGenerator()\n",
    "# ZCA whiteningなどを行う場合が以下の実行が必要\n",
    "# datagen.fit(train_x)\n",
    "# datagen_for_test.fit(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c166d223-419d-48ac-8bed-b6933a4bb475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.111439Z",
     "iopub.status.busy": "2023-04-19T11:20:56.111244Z",
     "iopub.status.idle": "2023-04-19T11:20:56.898338Z",
     "shell.execute_reply": "2023-04-19T11:20:56.897362Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global seed\n",
    "seed = model_seeds[0]\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# prepare data\n",
    "# dataset_loader = getattr(libs.utils, 'prepare_'+dataset)\n",
    "# to_categorical=False\n",
    "# (x_train, y_train), (x_test, y_test) = dataset_loader(to_categorical)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "dataset_test = (x_test, y_test) \n",
    "dataset_train = (x_train, y_train)\n",
    "\n",
    "x_train = dataset_train[0] / 255\n",
    "y_train = dataset_train[1].squeeze()\n",
    "x_test = dataset_test[0] / 255\n",
    "y_test = dataset_test[1].squeeze()\n",
    "\n",
    "number_train = x_train.shape[0]\n",
    "number_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e786c9dc-5600-4c9b-8605-6894eef91bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.905065Z",
     "iopub.status.busy": "2023-04-19T11:20:56.904874Z",
     "iopub.status.idle": "2023-04-19T11:20:56.912672Z",
     "shell.execute_reply": "2023-04-19T11:20:56.911593Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "class LearningController(Callback):\n",
    "    def __init__(self, num_epoch=0, learn_minute=0):\n",
    "        self.num_epoch = num_epoch\n",
    "        self.learn_second = learn_minute * 60\n",
    "        if self.learn_second > 0:\n",
    "            print(\"Learning rate is controled by time.\")\n",
    "        elif self.num_epoch > 0:\n",
    "            print(\"Learning rate is controled by epoch.\")\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.learn_second > 0:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.learn_second > 0:\n",
    "            current_time = time.time()\n",
    "            if current_time - self.start_time > self.learn_second:\n",
    "                self.model.stop_training = True\n",
    "                print(\"Time is up.\")\n",
    "                return\n",
    "\n",
    "            if current_time - self.start_time > self.learn_second / 2:\n",
    "                self.model.optimizer.lr = lr * 0.1            \n",
    "            if current_time - self.start_time > self.learn_second * 3 / 4:\n",
    "                self.model.optimizer.lr = lr * 0.01\n",
    "                \n",
    "        elif self.num_epoch > 0:\n",
    "            if epoch > self.num_epoch / 2:\n",
    "                self.model.optimizer.lr = lr * 0.1            \n",
    "            if epoch > self.num_epoch * 3 / 4:\n",
    "                self.model.optimizer.lr = lr * 0.01\n",
    "                    \n",
    "        print('\\nlr:%.2e' % self.model.optimizer.lr.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f5b823-46bf-4ada-9ea0-6864a8a1284d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:56.916685Z",
     "iopub.status.busy": "2023-04-19T11:20:56.916395Z",
     "iopub.status.idle": "2023-04-19T11:20:59.206986Z",
     "shell.execute_reply": "2023-04-19T11:20:59.206121Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Activation, MaxPool2D, GlobalAveragePooling2D, Add, Input, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "n_classes = y_train.shape\n",
    "\n",
    "# n_channels = x_train.shape[3] # no. of channels (?)\n",
    "# # Computed depth of model\n",
    "# if resnet_version == 1:\n",
    "# \tdepth = n_channels * 6 + 2\n",
    "# elif resnet_version == 2:\n",
    "# \tdepth = n_channels * 9 + 2\n",
    "\n",
    "# model_generator = getattr(libs.model_archs, model_arch)\n",
    "# model = model_generator(input_shape, depth, n_classes)\n",
    "\n",
    "# https://github.com/shoji9x9/CIFAR-10-By-small-ResNet/blob/master/ResNet-for-CIFAR-10-with-Keras.ipynb\n",
    "n = 9 # 56 layers\n",
    "channels = [16, 32, 64]\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(channels[0], \n",
    "           kernel_size=(3, 3), \n",
    "           padding=\"same\", \n",
    "           kernel_initializer=\"he_normal\", \n",
    "           kernel_regularizer=l2(1e-4))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(tf.nn.relu)(x)\n",
    "\n",
    "for c in channels:\n",
    "    for i in range(n):\n",
    "        subsampling = i == 0 and c > 16\n",
    "        strides = (2, 2) if subsampling else (1, 1)\n",
    "        y = Conv2D(c, \n",
    "                   kernel_size=(3, 3), \n",
    "                   padding=\"same\", \n",
    "                   strides=strides, \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   kernel_regularizer=l2(1e-4))(x)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation(tf.nn.relu)(y)\n",
    "        y = Conv2D(c, \n",
    "                   kernel_size=(3, 3), \n",
    "                   padding=\"same\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   kernel_regularizer=l2(1e-4))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        if subsampling:\n",
    "            x = Conv2D(c, \n",
    "                       kernel_size=(1, 1), \n",
    "                       strides=(2, 2), \n",
    "                       padding=\"same\", \n",
    "                       kernel_initializer=\"he_normal\", \n",
    "                       kernel_regularizer=l2(1e-4))(x)\n",
    "        x = Add()([x, y])\n",
    "        x = Activation(tf.nn.relu)(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(10, \n",
    "                activation=tf.nn.softmax, \n",
    "                kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model._name = \"resnet_lrs_checkpnt_cutout_rerun\" + str(6 * n + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05a8e80-363e-400b-b8e7-6f9d0e9eb4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:59.214138Z",
     "iopub.status.busy": "2023-04-19T11:20:59.213710Z",
     "iopub.status.idle": "2023-04-19T11:20:59.218991Z",
     "shell.execute_reply": "2023-04-19T11:20:59.217852Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(filepath = \"ResNet-for-CIFAR-10-with-Keras.h5\", \n",
    "                             monitor=\"val_loss\", \n",
    "                             verbose=1, \n",
    "                             save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72459af0-69f4-43c9-a3f3-7c8670b00359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T11:20:59.222511Z",
     "iopub.status.busy": "2023-04-19T11:20:59.222279Z",
     "iopub.status.idle": "2023-04-19T12:58:26.239702Z",
     "shell.execute_reply": "2023-04-19T12:58:26.238350Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is controled by epoch.\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 6.5922 - accuracy: 0.0999\n",
      "Epoch 1: val_loss improved from inf to 5.89357, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 43s 69ms/step - loss: 6.5922 - accuracy: 0.0999 - val_loss: 5.8936 - val_accuracy: 0.1002\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 5.6277 - accuracy: 0.1001\n",
      "Epoch 2: val_loss improved from 5.89357 to 5.37760, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 25s 63ms/step - loss: 5.6277 - accuracy: 0.1001 - val_loss: 5.3776 - val_accuracy: 0.1000\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 5.1466 - accuracy: 0.1001\n",
      "Epoch 3: val_loss improved from 5.37760 to 4.92868, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 24s 62ms/step - loss: 5.1466 - accuracy: 0.1001 - val_loss: 4.9287 - val_accuracy: 0.1001\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 4.7355 - accuracy: 0.0979\n",
      "Epoch 4: val_loss improved from 4.92868 to 4.55171, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 24s 62ms/step - loss: 4.7355 - accuracy: 0.0979 - val_loss: 4.5517 - val_accuracy: 0.1000\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 4.3839 - accuracy: 0.0987\n",
      "Epoch 5: val_loss improved from 4.55171 to 4.22456, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 24s 62ms/step - loss: 4.3839 - accuracy: 0.0987 - val_loss: 4.2246 - val_accuracy: 0.0997\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 4.0829 - accuracy: 0.0994\n",
      "Epoch 6: val_loss improved from 4.22456 to 3.94794, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 24s 61ms/step - loss: 4.0829 - accuracy: 0.0994 - val_loss: 3.9479 - val_accuracy: 0.1002\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 3.8255 - accuracy: 0.0992\n",
      "Epoch 7: val_loss improved from 3.94794 to 3.70975, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 24s 61ms/step - loss: 3.8255 - accuracy: 0.0992 - val_loss: 3.7097 - val_accuracy: 0.0999\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 3.6055 - accuracy: 0.0996\n",
      "Epoch 8: val_loss improved from 3.70975 to 3.50736, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 24s 61ms/step - loss: 3.6055 - accuracy: 0.0996 - val_loss: 3.5074 - val_accuracy: 0.1002\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 3.4173 - accuracy: 0.0999\n",
      "Epoch 9: val_loss improved from 3.50736 to 3.33184, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 3.4173 - accuracy: 0.0999 - val_loss: 3.3318 - val_accuracy: 0.1002\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 3.2564 - accuracy: 0.0994\n",
      "Epoch 10: val_loss improved from 3.33184 to 3.18298, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 3.2564 - accuracy: 0.0994 - val_loss: 3.1830 - val_accuracy: 0.1001\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 3.1185 - accuracy: 0.0992\n",
      "Epoch 11: val_loss improved from 3.18298 to 3.05597, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 3.1185 - accuracy: 0.0992 - val_loss: 3.0560 - val_accuracy: 0.1001\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 3.0007 - accuracy: 0.0983\n",
      "Epoch 12: val_loss improved from 3.05597 to 2.94860, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 3.0007 - accuracy: 0.0983 - val_loss: 2.9486 - val_accuracy: 0.1001\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.9006 - accuracy: 0.0973\n",
      "Epoch 13: val_loss improved from 2.94860 to 2.85567, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 2.9006 - accuracy: 0.0973 - val_loss: 2.8557 - val_accuracy: 0.1001\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.8146 - accuracy: 0.0994\n",
      "Epoch 14: val_loss improved from 2.85567 to 2.77532, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 2.8146 - accuracy: 0.0994 - val_loss: 2.7753 - val_accuracy: 0.1000\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.7406 - accuracy: 0.0973\n",
      "Epoch 15: val_loss improved from 2.77532 to 2.70631, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 80ms/step - loss: 2.7406 - accuracy: 0.0973 - val_loss: 2.7063 - val_accuracy: 0.1000\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.6774 - accuracy: 0.0984\n",
      "Epoch 16: val_loss improved from 2.70631 to 2.64905, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 2.6774 - accuracy: 0.0984 - val_loss: 2.6491 - val_accuracy: 0.1001\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.6273 - accuracy: 0.0994\n",
      "Epoch 17: val_loss improved from 2.64905 to 2.60393, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 2.6273 - accuracy: 0.0994 - val_loss: 2.6039 - val_accuracy: 0.1000\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.5806 - accuracy: 0.0984\n",
      "Epoch 18: val_loss improved from 2.60393 to 2.55891, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 2.5806 - accuracy: 0.0984 - val_loss: 2.5589 - val_accuracy: 0.0998\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.5406 - accuracy: 0.0979\n",
      "Epoch 19: val_loss improved from 2.55891 to 2.52140, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 2.5406 - accuracy: 0.0979 - val_loss: 2.5214 - val_accuracy: 0.1001\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.5066 - accuracy: 0.0995\n",
      "Epoch 20: val_loss improved from 2.52140 to 2.49061, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 2.5066 - accuracy: 0.0995 - val_loss: 2.4906 - val_accuracy: 0.1000\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.4772 - accuracy: 0.0985\n",
      "Epoch 21: val_loss improved from 2.49061 to 2.46301, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 2.4772 - accuracy: 0.0985 - val_loss: 2.4630 - val_accuracy: 0.0999\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.4520 - accuracy: 0.0977\n",
      "Epoch 22: val_loss improved from 2.46301 to 2.43977, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 2.4520 - accuracy: 0.0977 - val_loss: 2.4398 - val_accuracy: 0.1001\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.4306 - accuracy: 0.0994\n",
      "Epoch 23: val_loss improved from 2.43977 to 2.42209, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 2.4306 - accuracy: 0.0994 - val_loss: 2.4221 - val_accuracy: 0.0999\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.4124 - accuracy: 0.1007\n",
      "Epoch 24: val_loss improved from 2.42209 to 2.40448, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 2.4124 - accuracy: 0.1007 - val_loss: 2.4045 - val_accuracy: 0.1001\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.3970 - accuracy: 0.0987\n",
      "Epoch 25: val_loss improved from 2.40448 to 2.39092, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 2.3970 - accuracy: 0.0987 - val_loss: 2.3909 - val_accuracy: 0.1001\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.3835 - accuracy: 0.0986\n",
      "Epoch 26: val_loss improved from 2.39092 to 2.37925, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 2.3835 - accuracy: 0.0986 - val_loss: 2.3793 - val_accuracy: 0.0998\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.3726 - accuracy: 0.0991\n",
      "Epoch 27: val_loss improved from 2.37925 to 2.36716, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 2.3726 - accuracy: 0.0991 - val_loss: 2.3672 - val_accuracy: 0.1002\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 2.3167 - accuracy: 0.1256\n",
      "Epoch 28: val_loss did not improve from 2.36716\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 2.3167 - accuracy: 0.1256 - val_loss: 26.6685 - val_accuracy: 0.1290\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.9668 - accuracy: 0.2798\n",
      "Epoch 29: val_loss improved from 2.36716 to 2.06395, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 1.9668 - accuracy: 0.2798 - val_loss: 2.0639 - val_accuracy: 0.2825\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.7853 - accuracy: 0.3626\n",
      "Epoch 30: val_loss did not improve from 2.06395\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.7853 - accuracy: 0.3626 - val_loss: 2.8806 - val_accuracy: 0.2054\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.6940 - accuracy: 0.3973\n",
      "Epoch 31: val_loss did not improve from 2.06395\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.6940 - accuracy: 0.3973 - val_loss: 2.7287 - val_accuracy: 0.1726\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.6201 - accuracy: 0.4288\n",
      "Epoch 32: val_loss did not improve from 2.06395\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.6201 - accuracy: 0.4288 - val_loss: 2.9300 - val_accuracy: 0.2356\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.5265 - accuracy: 0.4697\n",
      "Epoch 33: val_loss did not improve from 2.06395\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.5265 - accuracy: 0.4697 - val_loss: 2.6365 - val_accuracy: 0.3505\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.4429 - accuracy: 0.5030\n",
      "Epoch 34: val_loss did not improve from 2.06395\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.4429 - accuracy: 0.5030 - val_loss: 2.8458 - val_accuracy: 0.3151\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.3813 - accuracy: 0.5275\n",
      "Epoch 35: val_loss improved from 2.06395 to 1.67646, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 1.3813 - accuracy: 0.5275 - val_loss: 1.6765 - val_accuracy: 0.4834\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.3249 - accuracy: 0.5570\n",
      "Epoch 36: val_loss did not improve from 1.67646\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.3249 - accuracy: 0.5570 - val_loss: 2.0884 - val_accuracy: 0.3905\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.2768 - accuracy: 0.5809\n",
      "Epoch 37: val_loss did not improve from 1.67646\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.2768 - accuracy: 0.5809 - val_loss: 2.1818 - val_accuracy: 0.3974\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.2261 - accuracy: 0.6020\n",
      "Epoch 38: val_loss did not improve from 1.67646\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.2261 - accuracy: 0.6020 - val_loss: 2.7658 - val_accuracy: 0.4010\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.1861 - accuracy: 0.6180\n",
      "Epoch 39: val_loss improved from 1.67646 to 1.31877, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.1861 - accuracy: 0.6180 - val_loss: 1.3188 - val_accuracy: 0.5873\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.1467 - accuracy: 0.6375\n",
      "Epoch 40: val_loss did not improve from 1.31877\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 1.1467 - accuracy: 0.6375 - val_loss: 1.6875 - val_accuracy: 0.4923\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.1331 - accuracy: 0.6467\n",
      "Epoch 41: val_loss improved from 1.31877 to 1.26936, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.1331 - accuracy: 0.6467 - val_loss: 1.2694 - val_accuracy: 0.6223\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.1060 - accuracy: 0.6586\n",
      "Epoch 42: val_loss did not improve from 1.26936\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 1.1060 - accuracy: 0.6586 - val_loss: 1.7747 - val_accuracy: 0.5327\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0852 - accuracy: 0.6696\n",
      "Epoch 43: val_loss did not improve from 1.26936\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.0852 - accuracy: 0.6696 - val_loss: 1.5583 - val_accuracy: 0.5999\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0703 - accuracy: 0.6754\n",
      "Epoch 44: val_loss did not improve from 1.26936\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.0703 - accuracy: 0.6754 - val_loss: 1.4034 - val_accuracy: 0.6083\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0486 - accuracy: 0.6854\n",
      "Epoch 45: val_loss improved from 1.26936 to 1.07885, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 1.0486 - accuracy: 0.6854 - val_loss: 1.0788 - val_accuracy: 0.6892\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0316 - accuracy: 0.6907\n",
      "Epoch 46: val_loss did not improve from 1.07885\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 1.0316 - accuracy: 0.6907 - val_loss: 1.5038 - val_accuracy: 0.5613\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0226 - accuracy: 0.6988\n",
      "Epoch 47: val_loss did not improve from 1.07885\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 1.0226 - accuracy: 0.6988 - val_loss: 1.1556 - val_accuracy: 0.6716\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0121 - accuracy: 0.7027\n",
      "Epoch 48: val_loss improved from 1.07885 to 0.99490, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 1.0121 - accuracy: 0.7027 - val_loss: 0.9949 - val_accuracy: 0.7181\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0051 - accuracy: 0.7071\n",
      "Epoch 49: val_loss did not improve from 0.99490\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 1.0051 - accuracy: 0.7071 - val_loss: 1.9558 - val_accuracy: 0.4993\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9849 - accuracy: 0.7174\n",
      "Epoch 50: val_loss improved from 0.99490 to 0.96487, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9849 - accuracy: 0.7174 - val_loss: 0.9649 - val_accuracy: 0.7408\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9814 - accuracy: 0.7206\n",
      "Epoch 51: val_loss did not improve from 0.96487\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.9814 - accuracy: 0.7206 - val_loss: 1.1166 - val_accuracy: 0.7093\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9714 - accuracy: 0.7236\n",
      "Epoch 52: val_loss did not improve from 0.96487\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.9714 - accuracy: 0.7236 - val_loss: 1.1794 - val_accuracy: 0.6599\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9580 - accuracy: 0.7322\n",
      "Epoch 53: val_loss did not improve from 0.96487\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9580 - accuracy: 0.7322 - val_loss: 1.1826 - val_accuracy: 0.6640\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9536 - accuracy: 0.7344\n",
      "Epoch 54: val_loss did not improve from 0.96487\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.9536 - accuracy: 0.7344 - val_loss: 1.6202 - val_accuracy: 0.6086\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9453 - accuracy: 0.7371\n",
      "Epoch 55: val_loss did not improve from 0.96487\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9453 - accuracy: 0.7371 - val_loss: 1.2541 - val_accuracy: 0.6797\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9365 - accuracy: 0.7406\n",
      "Epoch 56: val_loss improved from 0.96487 to 0.92640, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9365 - accuracy: 0.7406 - val_loss: 0.9264 - val_accuracy: 0.7507\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9266 - accuracy: 0.7490\n",
      "Epoch 57: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.9266 - accuracy: 0.7490 - val_loss: 0.9598 - val_accuracy: 0.7485\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9252 - accuracy: 0.7489\n",
      "Epoch 58: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9252 - accuracy: 0.7489 - val_loss: 1.3517 - val_accuracy: 0.6403\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9167 - accuracy: 0.7532\n",
      "Epoch 59: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9167 - accuracy: 0.7532 - val_loss: 1.1075 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9079 - accuracy: 0.7550\n",
      "Epoch 60: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.9079 - accuracy: 0.7550 - val_loss: 1.5833 - val_accuracy: 0.6053\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9078 - accuracy: 0.7568\n",
      "Epoch 61: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.9078 - accuracy: 0.7568 - val_loss: 1.4811 - val_accuracy: 0.6271\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8973 - accuracy: 0.7592\n",
      "Epoch 62: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8973 - accuracy: 0.7592 - val_loss: 1.0989 - val_accuracy: 0.7243\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8890 - accuracy: 0.7650\n",
      "Epoch 63: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8890 - accuracy: 0.7650 - val_loss: 1.1632 - val_accuracy: 0.7016\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8962 - accuracy: 0.7605\n",
      "Epoch 64: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8962 - accuracy: 0.7605 - val_loss: 1.1239 - val_accuracy: 0.7334\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8834 - accuracy: 0.7683\n",
      "Epoch 65: val_loss did not improve from 0.92640\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8834 - accuracy: 0.7683 - val_loss: 1.4037 - val_accuracy: 0.6672\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8833 - accuracy: 0.7672\n",
      "Epoch 66: val_loss improved from 0.92640 to 0.92595, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8833 - accuracy: 0.7672 - val_loss: 0.9260 - val_accuracy: 0.7676\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8753 - accuracy: 0.7731\n",
      "Epoch 67: val_loss did not improve from 0.92595\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8753 - accuracy: 0.7731 - val_loss: 1.2281 - val_accuracy: 0.6738\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8601 - accuracy: 0.7796\n",
      "Epoch 68: val_loss did not improve from 0.92595\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8601 - accuracy: 0.7796 - val_loss: 1.4356 - val_accuracy: 0.6455\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8626 - accuracy: 0.7799\n",
      "Epoch 69: val_loss did not improve from 0.92595\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8626 - accuracy: 0.7799 - val_loss: 0.9969 - val_accuracy: 0.7511\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.7779\n",
      "Epoch 70: val_loss did not improve from 0.92595\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 76ms/step - loss: 0.8658 - accuracy: 0.7779 - val_loss: 1.1225 - val_accuracy: 0.7219\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8509 - accuracy: 0.7826\n",
      "Epoch 71: val_loss did not improve from 0.92595\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8509 - accuracy: 0.7826 - val_loss: 1.4605 - val_accuracy: 0.6559\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8538 - accuracy: 0.7836\n",
      "Epoch 72: val_loss improved from 0.92595 to 0.88949, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8538 - accuracy: 0.7836 - val_loss: 0.8895 - val_accuracy: 0.7788\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8492 - accuracy: 0.7847\n",
      "Epoch 73: val_loss did not improve from 0.88949\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8492 - accuracy: 0.7847 - val_loss: 1.1483 - val_accuracy: 0.7180\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8488 - accuracy: 0.7864\n",
      "Epoch 74: val_loss improved from 0.88949 to 0.84364, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8488 - accuracy: 0.7864 - val_loss: 0.8436 - val_accuracy: 0.8000\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8444 - accuracy: 0.7869\n",
      "Epoch 75: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8444 - accuracy: 0.7869 - val_loss: 1.0683 - val_accuracy: 0.7463\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8383 - accuracy: 0.7894\n",
      "Epoch 76: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8383 - accuracy: 0.7894 - val_loss: 0.8906 - val_accuracy: 0.7894\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8369 - accuracy: 0.7935\n",
      "Epoch 77: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8369 - accuracy: 0.7935 - val_loss: 0.9156 - val_accuracy: 0.7785\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8375 - accuracy: 0.7906\n",
      "Epoch 78: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8375 - accuracy: 0.7906 - val_loss: 1.3580 - val_accuracy: 0.6746\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8340 - accuracy: 0.7927\n",
      "Epoch 79: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8340 - accuracy: 0.7927 - val_loss: 0.8949 - val_accuracy: 0.7832\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8404 - accuracy: 0.7920\n",
      "Epoch 80: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8404 - accuracy: 0.7920 - val_loss: 1.0633 - val_accuracy: 0.7426\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.7976\n",
      "Epoch 81: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8239 - accuracy: 0.7976 - val_loss: 0.9318 - val_accuracy: 0.7831\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8357 - accuracy: 0.7954\n",
      "Epoch 82: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8357 - accuracy: 0.7954 - val_loss: 1.6793 - val_accuracy: 0.6023\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8243 - accuracy: 0.7996\n",
      "Epoch 83: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8243 - accuracy: 0.7996 - val_loss: 1.0620 - val_accuracy: 0.7495\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8198 - accuracy: 0.8018\n",
      "Epoch 84: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8198 - accuracy: 0.8018 - val_loss: 0.9001 - val_accuracy: 0.7888\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8221 - accuracy: 0.8018\n",
      "Epoch 85: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8221 - accuracy: 0.8018 - val_loss: 0.9385 - val_accuracy: 0.7752\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8144 - accuracy: 0.8040\n",
      "Epoch 86: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8144 - accuracy: 0.8040 - val_loss: 0.9938 - val_accuracy: 0.7692\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8179 - accuracy: 0.8045\n",
      "Epoch 87: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8179 - accuracy: 0.8045 - val_loss: 1.1832 - val_accuracy: 0.7004\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8162 - accuracy: 0.8040\n",
      "Epoch 88: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8162 - accuracy: 0.8040 - val_loss: 1.1050 - val_accuracy: 0.7227\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8080 - accuracy: 0.8062\n",
      "Epoch 89: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8080 - accuracy: 0.8062 - val_loss: 0.8852 - val_accuracy: 0.7952\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8039 - accuracy: 0.8095\n",
      "Epoch 90: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8039 - accuracy: 0.8095 - val_loss: 1.2199 - val_accuracy: 0.7045\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8064 - accuracy: 0.8082\n",
      "Epoch 91: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8064 - accuracy: 0.8082 - val_loss: 0.9944 - val_accuracy: 0.7712\n",
      "Epoch 92/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8012 - accuracy: 0.8114\n",
      "Epoch 92: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8012 - accuracy: 0.8114 - val_loss: 0.9335 - val_accuracy: 0.7767\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8114 - accuracy: 0.8070\n",
      "Epoch 93: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.8114 - accuracy: 0.8070 - val_loss: 2.2122 - val_accuracy: 0.4988\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7976 - accuracy: 0.8103\n",
      "Epoch 94: val_loss did not improve from 0.84364\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.7976 - accuracy: 0.8103 - val_loss: 1.6591 - val_accuracy: 0.5899\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8015 - accuracy: 0.8104\n",
      "Epoch 95: val_loss improved from 0.84364 to 0.78886, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.8015 - accuracy: 0.8104 - val_loss: 0.7889 - val_accuracy: 0.8263\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7969 - accuracy: 0.8116\n",
      "Epoch 96: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.7969 - accuracy: 0.8116 - val_loss: 1.4613 - val_accuracy: 0.6658\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7966 - accuracy: 0.8121\n",
      "Epoch 97: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.7966 - accuracy: 0.8121 - val_loss: 0.9611 - val_accuracy: 0.7882\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8023 - accuracy: 0.8116\n",
      "Epoch 98: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.8023 - accuracy: 0.8116 - val_loss: 0.9586 - val_accuracy: 0.7737\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7927 - accuracy: 0.8164\n",
      "Epoch 99: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.7927 - accuracy: 0.8164 - val_loss: 0.9815 - val_accuracy: 0.7633\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.8173\n",
      "Epoch 100: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.7912 - accuracy: 0.8173 - val_loss: 0.9984 - val_accuracy: 0.7602\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7924 - accuracy: 0.8157\n",
      "Epoch 101: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-01\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.7924 - accuracy: 0.8157 - val_loss: 0.8583 - val_accuracy: 0.8040\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7892 - accuracy: 0.8183\n",
      "Epoch 102: val_loss did not improve from 0.78886\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.7892 - accuracy: 0.8183 - val_loss: 0.9306 - val_accuracy: 0.7766\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6914 - accuracy: 0.8507\n",
      "Epoch 103: val_loss improved from 0.78886 to 0.61118, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.6914 - accuracy: 0.8507 - val_loss: 0.6112 - val_accuracy: 0.8840\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.8686\n",
      "Epoch 104: val_loss did not improve from 0.61118\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.6392 - accuracy: 0.8686 - val_loss: 0.6113 - val_accuracy: 0.8826\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.8745\n",
      "Epoch 105: val_loss improved from 0.61118 to 0.61114, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.6153 - accuracy: 0.8745 - val_loss: 0.6111 - val_accuracy: 0.8819\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.8766\n",
      "Epoch 106: val_loss improved from 0.61114 to 0.59155, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.6033 - accuracy: 0.8766 - val_loss: 0.5916 - val_accuracy: 0.8885\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5848 - accuracy: 0.8817\n",
      "Epoch 107: val_loss improved from 0.59155 to 0.57803, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.5848 - accuracy: 0.8817 - val_loss: 0.5780 - val_accuracy: 0.8922\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.8819\n",
      "Epoch 108: val_loss improved from 0.57803 to 0.56099, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.5838 - accuracy: 0.8819 - val_loss: 0.5610 - val_accuracy: 0.8969\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.8857\n",
      "Epoch 109: val_loss did not improve from 0.56099\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.5646 - accuracy: 0.8857 - val_loss: 0.6138 - val_accuracy: 0.8786\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5501 - accuracy: 0.8913\n",
      "Epoch 110: val_loss improved from 0.56099 to 0.54577, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.5501 - accuracy: 0.8913 - val_loss: 0.5458 - val_accuracy: 0.8985\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.8919\n",
      "Epoch 111: val_loss improved from 0.54577 to 0.54243, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.5477 - accuracy: 0.8919 - val_loss: 0.5424 - val_accuracy: 0.8975\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.8924\n",
      "Epoch 112: val_loss did not improve from 0.54243\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.5417 - accuracy: 0.8924 - val_loss: 0.5778 - val_accuracy: 0.8876\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.8927\n",
      "Epoch 113: val_loss did not improve from 0.54243\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.5361 - accuracy: 0.8927 - val_loss: 0.5593 - val_accuracy: 0.8915\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.8945\n",
      "Epoch 114: val_loss did not improve from 0.54243\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.5279 - accuracy: 0.8945 - val_loss: 0.5471 - val_accuracy: 0.8955\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.8938\n",
      "Epoch 115: val_loss improved from 0.54243 to 0.53723, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.5262 - accuracy: 0.8938 - val_loss: 0.5372 - val_accuracy: 0.8989\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.8997\n",
      "Epoch 116: val_loss did not improve from 0.53723\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.5099 - accuracy: 0.8997 - val_loss: 0.5471 - val_accuracy: 0.8964\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.8984\n",
      "Epoch 117: val_loss did not improve from 0.53723\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.5097 - accuracy: 0.8984 - val_loss: 0.5443 - val_accuracy: 0.8940\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.8971\n",
      "Epoch 118: val_loss improved from 0.53723 to 0.52367, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.5054 - accuracy: 0.8971 - val_loss: 0.5237 - val_accuracy: 0.9004\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.8989\n",
      "Epoch 119: val_loss did not improve from 0.52367\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.4975 - accuracy: 0.8989 - val_loss: 0.5342 - val_accuracy: 0.8963\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.9011\n",
      "Epoch 120: val_loss did not improve from 0.52367\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.4909 - accuracy: 0.9011 - val_loss: 0.5302 - val_accuracy: 0.8960\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.9011\n",
      "Epoch 121: val_loss improved from 0.52367 to 0.50657, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.4886 - accuracy: 0.9011 - val_loss: 0.5066 - val_accuracy: 0.9022\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.9034\n",
      "Epoch 122: val_loss did not improve from 0.50657\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.4803 - accuracy: 0.9034 - val_loss: 0.5145 - val_accuracy: 0.9006\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.9032\n",
      "Epoch 123: val_loss did not improve from 0.50657\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.4808 - accuracy: 0.9032 - val_loss: 0.5271 - val_accuracy: 0.8948\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.9048\n",
      "Epoch 124: val_loss did not improve from 0.50657\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.4714 - accuracy: 0.9048 - val_loss: 0.5208 - val_accuracy: 0.8993\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4683 - accuracy: 0.9048\n",
      "Epoch 125: val_loss improved from 0.50657 to 0.49659, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 30s 77ms/step - loss: 0.4683 - accuracy: 0.9048 - val_loss: 0.4966 - val_accuracy: 0.9041\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.9044\n",
      "Epoch 126: val_loss did not improve from 0.49659\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4676 - accuracy: 0.9044 - val_loss: 0.5340 - val_accuracy: 0.8921\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4627 - accuracy: 0.9060\n",
      "Epoch 127: val_loss did not improve from 0.49659\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4627 - accuracy: 0.9060 - val_loss: 0.5111 - val_accuracy: 0.8985\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.9072\n",
      "Epoch 128: val_loss did not improve from 0.49659\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4556 - accuracy: 0.9072 - val_loss: 0.5297 - val_accuracy: 0.8916\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.9069\n",
      "Epoch 129: val_loss did not improve from 0.49659\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4535 - accuracy: 0.9069 - val_loss: 0.5235 - val_accuracy: 0.8947\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.9088\n",
      "Epoch 130: val_loss did not improve from 0.49659\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4461 - accuracy: 0.9088 - val_loss: 0.5178 - val_accuracy: 0.8951\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.9090\n",
      "Epoch 131: val_loss improved from 0.49659 to 0.49597, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4416 - accuracy: 0.9090 - val_loss: 0.4960 - val_accuracy: 0.9009\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.9079\n",
      "Epoch 132: val_loss did not improve from 0.49597\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.4408 - accuracy: 0.9079 - val_loss: 0.5185 - val_accuracy: 0.8942\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.9084\n",
      "Epoch 133: val_loss did not improve from 0.49597\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4440 - accuracy: 0.9084 - val_loss: 0.5137 - val_accuracy: 0.8940\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4389 - accuracy: 0.9075\n",
      "Epoch 134: val_loss did not improve from 0.49597\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.4389 - accuracy: 0.9075 - val_loss: 0.5030 - val_accuracy: 0.8973\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.9105\n",
      "Epoch 135: val_loss did not improve from 0.49597\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4324 - accuracy: 0.9105 - val_loss: 0.4973 - val_accuracy: 0.8981\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.9098\n",
      "Epoch 136: val_loss did not improve from 0.49597\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4289 - accuracy: 0.9098 - val_loss: 0.5135 - val_accuracy: 0.8969\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.9110\n",
      "Epoch 137: val_loss improved from 0.49597 to 0.48488, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.4261 - accuracy: 0.9110 - val_loss: 0.4849 - val_accuracy: 0.8990\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.9128\n",
      "Epoch 138: val_loss improved from 0.48488 to 0.47682, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.4221 - accuracy: 0.9128 - val_loss: 0.4768 - val_accuracy: 0.9038\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.9121\n",
      "Epoch 139: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.4177 - accuracy: 0.9121 - val_loss: 0.5115 - val_accuracy: 0.8992\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.9137\n",
      "Epoch 140: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.4130 - accuracy: 0.9137 - val_loss: 0.5363 - val_accuracy: 0.8822\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4082 - accuracy: 0.9153\n",
      "Epoch 141: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4082 - accuracy: 0.9153 - val_loss: 0.5052 - val_accuracy: 0.8984\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4148 - accuracy: 0.9126\n",
      "Epoch 142: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4148 - accuracy: 0.9126 - val_loss: 0.5118 - val_accuracy: 0.8912\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.9146\n",
      "Epoch 143: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4109 - accuracy: 0.9146 - val_loss: 0.5267 - val_accuracy: 0.8908\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4031 - accuracy: 0.9160\n",
      "Epoch 144: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4031 - accuracy: 0.9160 - val_loss: 0.5228 - val_accuracy: 0.8888\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.9128\n",
      "Epoch 145: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.4083 - accuracy: 0.9128 - val_loss: 0.4840 - val_accuracy: 0.9009\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.9136\n",
      "Epoch 146: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.4055 - accuracy: 0.9136 - val_loss: 0.4770 - val_accuracy: 0.8982\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.9138\n",
      "Epoch 147: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3995 - accuracy: 0.9138 - val_loss: 0.4991 - val_accuracy: 0.8930\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.9161\n",
      "Epoch 148: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3988 - accuracy: 0.9161 - val_loss: 0.5259 - val_accuracy: 0.8859\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.9165\n",
      "Epoch 149: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3948 - accuracy: 0.9165 - val_loss: 0.5167 - val_accuracy: 0.8928\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3989 - accuracy: 0.9138\n",
      "Epoch 150: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3989 - accuracy: 0.9138 - val_loss: 0.5025 - val_accuracy: 0.8930\n",
      "Epoch 151/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.9161\n",
      "Epoch 151: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-02\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3929 - accuracy: 0.9161 - val_loss: 0.5795 - val_accuracy: 0.8786\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3881 - accuracy: 0.9171\n",
      "Epoch 152: val_loss did not improve from 0.47682\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3881 - accuracy: 0.9171 - val_loss: 0.5124 - val_accuracy: 0.8933\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.9251\n",
      "Epoch 153: val_loss improved from 0.47682 to 0.43340, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.3691 - accuracy: 0.9251 - val_loss: 0.4334 - val_accuracy: 0.9133\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3552 - accuracy: 0.9297\n",
      "Epoch 154: val_loss improved from 0.43340 to 0.43298, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 30s 76ms/step - loss: 0.3552 - accuracy: 0.9297 - val_loss: 0.4330 - val_accuracy: 0.9133\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.9317\n",
      "Epoch 155: val_loss did not improve from 0.43298\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3506 - accuracy: 0.9317 - val_loss: 0.4351 - val_accuracy: 0.9128\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.9329\n",
      "Epoch 156: val_loss did not improve from 0.43298\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3441 - accuracy: 0.9329 - val_loss: 0.4421 - val_accuracy: 0.9128\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.9321\n",
      "Epoch 157: val_loss did not improve from 0.43298\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3456 - accuracy: 0.9321 - val_loss: 0.4387 - val_accuracy: 0.9126\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.9342\n",
      "Epoch 158: val_loss did not improve from 0.43298\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3418 - accuracy: 0.9342 - val_loss: 0.4342 - val_accuracy: 0.9118\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.9344\n",
      "Epoch 159: val_loss improved from 0.43298 to 0.43259, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3368 - accuracy: 0.9344 - val_loss: 0.4326 - val_accuracy: 0.9147\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.9334\n",
      "Epoch 160: val_loss did not improve from 0.43259\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3373 - accuracy: 0.9334 - val_loss: 0.4361 - val_accuracy: 0.9134\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.9365\n",
      "Epoch 161: val_loss did not improve from 0.43259\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3340 - accuracy: 0.9365 - val_loss: 0.4332 - val_accuracy: 0.9137\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.9374\n",
      "Epoch 162: val_loss did not improve from 0.43259\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3340 - accuracy: 0.9374 - val_loss: 0.4347 - val_accuracy: 0.9146\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.9362\n",
      "Epoch 163: val_loss did not improve from 0.43259\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3307 - accuracy: 0.9362 - val_loss: 0.4435 - val_accuracy: 0.9121\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.9377\n",
      "Epoch 164: val_loss did not improve from 0.43259\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3297 - accuracy: 0.9377 - val_loss: 0.4391 - val_accuracy: 0.9138\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.9386\n",
      "Epoch 165: val_loss did not improve from 0.43259\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3271 - accuracy: 0.9386 - val_loss: 0.4343 - val_accuracy: 0.9127\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.9369\n",
      "Epoch 166: val_loss improved from 0.43259 to 0.43203, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3279 - accuracy: 0.9369 - val_loss: 0.4320 - val_accuracy: 0.9128\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.9376\n",
      "Epoch 167: val_loss improved from 0.43203 to 0.43097, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.3253 - accuracy: 0.9376 - val_loss: 0.4310 - val_accuracy: 0.9134\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.9373\n",
      "Epoch 168: val_loss did not improve from 0.43097\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3242 - accuracy: 0.9373 - val_loss: 0.4366 - val_accuracy: 0.9143\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.9391\n",
      "Epoch 169: val_loss did not improve from 0.43097\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3267 - accuracy: 0.9391 - val_loss: 0.4374 - val_accuracy: 0.9146\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.9397\n",
      "Epoch 170: val_loss did not improve from 0.43097\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 72ms/step - loss: 0.3219 - accuracy: 0.9397 - val_loss: 0.4385 - val_accuracy: 0.9130\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.9378\n",
      "Epoch 171: val_loss improved from 0.43097 to 0.43023, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3246 - accuracy: 0.9378 - val_loss: 0.4302 - val_accuracy: 0.9162\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.9381\n",
      "Epoch 172: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 72ms/step - loss: 0.3221 - accuracy: 0.9381 - val_loss: 0.4314 - val_accuracy: 0.9143\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9406\n",
      "Epoch 173: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3201 - accuracy: 0.9406 - val_loss: 0.4341 - val_accuracy: 0.9154\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9391\n",
      "Epoch 174: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3201 - accuracy: 0.9391 - val_loss: 0.4357 - val_accuracy: 0.9157\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.9407\n",
      "Epoch 175: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3151 - accuracy: 0.9407 - val_loss: 0.4332 - val_accuracy: 0.9153\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3215 - accuracy: 0.9392\n",
      "Epoch 176: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3215 - accuracy: 0.9392 - val_loss: 0.4379 - val_accuracy: 0.9126\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.9425\n",
      "Epoch 177: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3162 - accuracy: 0.9425 - val_loss: 0.4368 - val_accuracy: 0.9155\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.9424\n",
      "Epoch 178: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3134 - accuracy: 0.9424 - val_loss: 0.4442 - val_accuracy: 0.9131\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3192 - accuracy: 0.9400\n",
      "Epoch 179: val_loss did not improve from 0.43023\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3192 - accuracy: 0.9400 - val_loss: 0.4316 - val_accuracy: 0.9157\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9409\n",
      "Epoch 180: val_loss improved from 0.43023 to 0.42982, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.3145 - accuracy: 0.9409 - val_loss: 0.4298 - val_accuracy: 0.9161\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.9421\n",
      "Epoch 181: val_loss did not improve from 0.42982\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3139 - accuracy: 0.9421 - val_loss: 0.4309 - val_accuracy: 0.9151\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.9420\n",
      "Epoch 182: val_loss improved from 0.42982 to 0.42966, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.3113 - accuracy: 0.9420 - val_loss: 0.4297 - val_accuracy: 0.9171\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3159 - accuracy: 0.9414\n",
      "Epoch 183: val_loss did not improve from 0.42966\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3159 - accuracy: 0.9414 - val_loss: 0.4362 - val_accuracy: 0.9124\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.9421\n",
      "Epoch 184: val_loss did not improve from 0.42966\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3122 - accuracy: 0.9421 - val_loss: 0.4322 - val_accuracy: 0.9123\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9411\n",
      "Epoch 185: val_loss did not improve from 0.42966\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3118 - accuracy: 0.9411 - val_loss: 0.4334 - val_accuracy: 0.9139\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.9440\n",
      "Epoch 186: val_loss did not improve from 0.42966\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3092 - accuracy: 0.9440 - val_loss: 0.4365 - val_accuracy: 0.9123\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9435\n",
      "Epoch 187: val_loss improved from 0.42966 to 0.42930, saving model to ResNet-for-CIFAR-10-with-Keras.h5\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 75ms/step - loss: 0.3068 - accuracy: 0.9435 - val_loss: 0.4293 - val_accuracy: 0.9160\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9448\n",
      "Epoch 188: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3047 - accuracy: 0.9448 - val_loss: 0.4383 - val_accuracy: 0.9144\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9427\n",
      "Epoch 189: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3118 - accuracy: 0.9427 - val_loss: 0.4314 - val_accuracy: 0.9135\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.9432\n",
      "Epoch 190: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3072 - accuracy: 0.9432 - val_loss: 0.4350 - val_accuracy: 0.9140\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.9442\n",
      "Epoch 191: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3061 - accuracy: 0.9442 - val_loss: 0.4416 - val_accuracy: 0.9142\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.9449\n",
      "Epoch 192: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3027 - accuracy: 0.9449 - val_loss: 0.4355 - val_accuracy: 0.9141\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.9452\n",
      "Epoch 193: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3079 - accuracy: 0.9452 - val_loss: 0.4324 - val_accuracy: 0.9139\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9434\n",
      "Epoch 194: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3057 - accuracy: 0.9434 - val_loss: 0.4340 - val_accuracy: 0.9151\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9435\n",
      "Epoch 195: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.3057 - accuracy: 0.9435 - val_loss: 0.4352 - val_accuracy: 0.9159\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.9450\n",
      "Epoch 196: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3032 - accuracy: 0.9450 - val_loss: 0.4394 - val_accuracy: 0.9149\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9462\n",
      "Epoch 197: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3000 - accuracy: 0.9462 - val_loss: 0.4381 - val_accuracy: 0.9140\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.9442\n",
      "Epoch 198: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 28s 73ms/step - loss: 0.3042 - accuracy: 0.9442 - val_loss: 0.4337 - val_accuracy: 0.9149\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9453\n",
      "Epoch 199: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 74ms/step - loss: 0.3008 - accuracy: 0.9453 - val_loss: 0.4367 - val_accuracy: 0.9152\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9460\n",
      "Epoch 200: val_loss did not improve from 0.42930\n",
      "\n",
      "lr:1.00e-03\n",
      "390/390 [==============================] - 29s 73ms/step - loss: 0.2994 - accuracy: 0.9460 - val_loss: 0.4400 - val_accuracy: 0.9129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f870c2ad940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# model.compile(loss ='categorical_crossentropy',\n",
    "#               optimizer ='adam',\n",
    "#               metrics =['accuracy'])\n",
    "lr = 0.1\n",
    "optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "learning_controller = LearningController(n_epochs)\n",
    "callbacks = [checkpoint, learning_controller]\n",
    "# train model\n",
    "model.fit(datagen.flow(x_train, \n",
    "                         y_train, \n",
    "                         batch_size=batch_size), \n",
    "          epochs=n_epochs,\n",
    "          steps_per_epoch=number_train//batch_size,\n",
    "          validation_data=datagen_for_test.flow(x_test, \n",
    "                                                  y_test, \n",
    "                                                  batch_size=batch_size),\n",
    "          validation_steps=number_test//batch_size,\n",
    "          validation_split=0.2,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9b8fda-5395-4334-8e95-dd7029b558d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T12:58:26.245010Z",
     "iopub.status.busy": "2023-04-19T12:58:26.244714Z",
     "iopub.status.idle": "2023-04-19T12:58:28.645978Z",
     "shell.execute_reply": "2023-04-19T12:58:28.644697Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 25ms/step - loss: 0.4395 - accuracy: 0.9130\n",
      "Original Accuracy:  [0.43954581022262573, 0.9129999876022339]\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, \n",
    "                       y_test, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "print(\"Original Accuracy: \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4160444-eaa9-47a9-831d-db3e89cd0e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T12:58:28.651531Z",
     "iopub.status.busy": "2023-04-19T12:58:28.651183Z",
     "iopub.status.idle": "2023-04-19T12:58:29.118176Z",
     "shell.execute_reply": "2023-04-19T12:58:29.116401Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model_type = dataset + \"--\" + model_arch\n",
    "model_instance = model_type + \"-\" + str(seed)\n",
    "model_filename = model_instance + \".h5\"\n",
    "model_subdir = pathlib.Path(MODELS_FOLDER / model.name)\n",
    "pathlib.Path(model_subdir).mkdir(parents=True, exist_ok=True)\n",
    "model_file = str(pathlib.Path(model_subdir/ model_filename))\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e11d853e-0eb6-432c-b767-473adc0dad2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T12:58:29.127093Z",
     "iopub.status.busy": "2023-04-19T12:58:29.126574Z",
     "iopub.status.idle": "2023-04-19T12:58:29.387833Z",
     "shell.execute_reply": "2023-04-19T12:58:29.386071Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_lrs_checkpnt_cutout_rerun56\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['activation_6[0][0]',           \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 16)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 16)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 16)  64          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 16)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 16)  64          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['activation_12[0][0]',          \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 16)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 16)  64          ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 16)  64          ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 32, 32, 16)   0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 32, 32, 16)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 16)  64          ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 16)  64          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 32, 16)   0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 16)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   544         ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 32)  128         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['conv2d_21[0][0]',              \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 32)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 32)  128         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['activation_20[0][0]',          \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 32)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 32)  128         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['activation_22[0][0]',          \n",
      "                                                                  'batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 32)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 32)  128         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['activation_24[0][0]',          \n",
      "                                                                  'batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['activation_26[0][0]',          \n",
      "                                                                  'batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['activation_28[0][0]',          \n",
      "                                                                  'batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 16, 16, 32)   0           ['activation_30[0][0]',          \n",
      "                                                                  'batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 16, 16, 32)   0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 16, 16, 32)  128         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 16, 16, 32)   0           ['activation_32[0][0]',          \n",
      "                                                                  'batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 16, 16, 32)   0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 16, 32)  128         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 16, 32)  128         ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16, 16, 32)   0           ['activation_34[0][0]',          \n",
      "                                                                  'batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 16, 16, 32)   0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['conv2d_40[0][0]',              \n",
      "                                                                  'batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 64)     0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['activation_38[0][0]',          \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 8, 8, 64)     0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 8, 8, 64)    256         ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['activation_40[0][0]',          \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 8, 8, 64)    256         ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 8, 64)    256         ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 8, 8, 64)     0           ['activation_42[0][0]',          \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 8, 8, 64)     0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 8, 64)    256         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 8, 64)    256         ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 8, 8, 64)     0           ['activation_44[0][0]',          \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 8, 8, 64)     0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 8, 64)    256         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 8, 8, 64)    256         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 8, 8, 64)     0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 8, 8, 64)     0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 8, 8, 64)    256         ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 64)    256         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['activation_48[0][0]',          \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['activation_50[0][0]',          \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['activation_52[0][0]',          \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_54[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 861,770\n",
      "Trainable params: 857,706\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951abcd3-78dc-4b84-ac86-50c46b3cf419",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
